{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat datang di Puja Repository \u00b6 Selamat datang di repository saya, di sini saya akan membagikan semua materi perkuliahan tentang Teknik Informatika. Seputar tentang Perkuliahan Teknik Informatika! \u00b6 Didalam Program Studi ini, dimana kita mempelajari dan menerapkan prinsip-prinsip ilmu komputer dan analisa matematis untuk pengembangan, desain, pengujian, dan evaluasi perangkat lunak, sistem operasi dan kerja komputer. Pada prodi ini akan diajarkan mulai dari algoritma, struktur data, desain, multimedia, database, rancangan program, rekayasa program, dan masih banyak lagi. Disini kita dituntut untuk menguasainya dan harus mempunyai banyak ide kreatif dimana di berbagai tugas kita di tuntut untuk membuat sebuah program / desain beda tiap mahasiswa.","title":"Home"},{"location":"#selamat-datang-di-puja-repository","text":"Selamat datang di repository saya, di sini saya akan membagikan semua materi perkuliahan tentang Teknik Informatika.","title":"Selamat datang di Puja Repository"},{"location":"#seputar-tentang-perkuliahan-teknik-informatika","text":"Didalam Program Studi ini, dimana kita mempelajari dan menerapkan prinsip-prinsip ilmu komputer dan analisa matematis untuk pengembangan, desain, pengujian, dan evaluasi perangkat lunak, sistem operasi dan kerja komputer. Pada prodi ini akan diajarkan mulai dari algoritma, struktur data, desain, multimedia, database, rancangan program, rekayasa program, dan masih banyak lagi. Disini kita dituntut untuk menguasainya dan harus mempunyai banyak ide kreatif dimana di berbagai tugas kita di tuntut untuk membuat sebuah program / desain beda tiap mahasiswa.","title":"Seputar tentang Perkuliahan Teknik Informatika!"},{"location":"semester-5/","text":"Mata Kuliah Semester 5 \u00b6","title":"Matakuliah Semester 5"},{"location":"semester-5/#mata-kuliah-semester-5","text":"","title":"Mata Kuliah Semester 5"},{"location":"semester-5/datamining/","text":"Apa itu Datamining? \u00b6 Datamining disebut juga sebagai Penambangan Data. Penambangan Data sendiri adalah proses ekstrasi pola yang menarik dari data dalam jumlah besar yang melibatkan penggunaan metode machine learning , statistik , dan sistem basis data . Tugas penambangan data aktual adalah analisis semi-otomatis atau otomatis dari sejumlah besar data untuk mengekstraksi pola yang sebelumnya tidak diketahui, contoh pola - pola menarik seperti : kelompok catatan data (cluster analysis) , catatan tidak biasa (anomali detection) , dan dependencies (asosiasi aturan penambangan , penambangan pola berurutan) . ini biasanya melibatkan penggunaan teknik basis data seperti indeks spasial . Pola yang disajikan haruslah mudah di pahami, berlaku untuk data yang akan di prediksi dengan derajat kepastian tertentu, berguna, dan baru. Proses Pencarian Pola (Patern) \u00b6 Penambangan data adalah salah satu bagian dari proses pencarian pola. Berikut ini urutan proses pencarian pola: Pembersihan Data : yaitu menghapus data pengganggu (noise) dan mengisi data yang hilang. Integrasi Data : yaitu menggabungkan berbagai sumber data. Pemilihan Data : yaitu memilih data yang relevan. Transformasi Data : yaitu mentransformasikan data ke dalam format untuk di proses dalam penambangan data. Penambangan Data : yaitu menerapkan metode cerdas untuk ekstraksi pola. Evaluasi Pola : yaitu mengenali pola - pola yang menarik saja. Penyajian Pola : yaitu memvisualisasikan pola ke pengguna. Teknik Penambangan Data \u00b6 Pada dasarnya penambangan data di bedakan menjadi dua fungsionalitas, yaitu deskripsi dan prediksi. Berikut beberapa fungsionalitas penambangan data yang sering digunakan: Karakteristik dan Diskriminasi : menggeneralisasi, merangkum, dan mengkontraskan karakteristik data Penambangan pola berulang : pencarian data dari suatu transaksi yang berulang Klasifikasi : membangun suatu model yang bisa mengkasifikasikan suatu objek berdasarkan atribut-atributnya. Prediksi : memprediksi nilai yang tidak di ketahui atau nilai yang hilang, menggunakan model dari klasifikasi. Penggugusan / Cluster Analysis : pengkelompokan kumpulan objek data berdasarkan kemiripannya. Analisis Outlier : proses mengenali noise dan pengecualian dalam data. Analisis Trend dan Evolusi : meliputi analisis regresi, penambangan pola sekuensial, analisis periode, dan analisis berbasis kemiripan. Referensi https://en.wikipedia.org/wiki/Data_mining https://id.wikipedia.org/wiki/Penggalian_data","title":"Apa itu Datamining?"},{"location":"semester-5/datamining/#apa-itu-datamining","text":"Datamining disebut juga sebagai Penambangan Data. Penambangan Data sendiri adalah proses ekstrasi pola yang menarik dari data dalam jumlah besar yang melibatkan penggunaan metode machine learning , statistik , dan sistem basis data . Tugas penambangan data aktual adalah analisis semi-otomatis atau otomatis dari sejumlah besar data untuk mengekstraksi pola yang sebelumnya tidak diketahui, contoh pola - pola menarik seperti : kelompok catatan data (cluster analysis) , catatan tidak biasa (anomali detection) , dan dependencies (asosiasi aturan penambangan , penambangan pola berurutan) . ini biasanya melibatkan penggunaan teknik basis data seperti indeks spasial . Pola yang disajikan haruslah mudah di pahami, berlaku untuk data yang akan di prediksi dengan derajat kepastian tertentu, berguna, dan baru.","title":"Apa itu Datamining?"},{"location":"semester-5/datamining/#proses-pencarian-pola-patern","text":"Penambangan data adalah salah satu bagian dari proses pencarian pola. Berikut ini urutan proses pencarian pola: Pembersihan Data : yaitu menghapus data pengganggu (noise) dan mengisi data yang hilang. Integrasi Data : yaitu menggabungkan berbagai sumber data. Pemilihan Data : yaitu memilih data yang relevan. Transformasi Data : yaitu mentransformasikan data ke dalam format untuk di proses dalam penambangan data. Penambangan Data : yaitu menerapkan metode cerdas untuk ekstraksi pola. Evaluasi Pola : yaitu mengenali pola - pola yang menarik saja. Penyajian Pola : yaitu memvisualisasikan pola ke pengguna.","title":"Proses Pencarian Pola (Patern)"},{"location":"semester-5/datamining/#teknik-penambangan-data","text":"Pada dasarnya penambangan data di bedakan menjadi dua fungsionalitas, yaitu deskripsi dan prediksi. Berikut beberapa fungsionalitas penambangan data yang sering digunakan: Karakteristik dan Diskriminasi : menggeneralisasi, merangkum, dan mengkontraskan karakteristik data Penambangan pola berulang : pencarian data dari suatu transaksi yang berulang Klasifikasi : membangun suatu model yang bisa mengkasifikasikan suatu objek berdasarkan atribut-atributnya. Prediksi : memprediksi nilai yang tidak di ketahui atau nilai yang hilang, menggunakan model dari klasifikasi. Penggugusan / Cluster Analysis : pengkelompokan kumpulan objek data berdasarkan kemiripannya. Analisis Outlier : proses mengenali noise dan pengecualian dalam data. Analisis Trend dan Evolusi : meliputi analisis regresi, penambangan pola sekuensial, analisis periode, dan analisis berbasis kemiripan. Referensi https://en.wikipedia.org/wiki/Data_mining https://id.wikipedia.org/wiki/Penggalian_data","title":"Teknik Penambangan Data"},{"location":"semester-5/datamining/pt2/","text":"Pertemuan ke-2 \u00b6 Pada perkuliahan pertemuan kedua saya mendapatkan materi tentang Statistika Deskriptif . didalam perkuliahan ini saya diajarkan dengan berbagai metode - metode seperti mencari Mean, Median, Standart Deviasi, Skewness, dan Quartile. Statistik Deskriptif \u00b6 Pengertian \u00b6 Statistika Deskriptif adalah metode - metode yang berkaitan dengan pengumpulan dan penyajian suatu gugus data sehingga memberikan informasi yang berguna. Statistika deskriptif hanya memberikan informasi mengenai data yang dipunyai dan sama sekali tidak menarik inferensia atau kesimpulan apapun tentang gugus induknya yang lebih besar. Contoh statistika deskriptif yang sering muncul adalah, tabel, diagram, grafik, dan besaran-besaran lain di majalah dan koran-koran. Dengan Statistika deskriptif, kumpulan data yang diperoleh akan tersaji dengan ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada. Informasi yang dapat diperoleh dari statistika deskriptif ini antara lain ukuran pemusatan data, ukuran penyebaran data, serta kecenderungan suatu gugus data. Statistika Deskriptif adalah statistika yang menggunakan data pada suatu kelompok untuk menjelaskan atau menarik suatu kesimpulan mengenai kelompok itu saja. Kelompok sendiri terdapat 3 ukuran yaitu Ukuran Lokasi, Ukuran Variabilitas, dan Ukuran Bentuk . Pada materi yang saya dapatkan sebagai berikut: Ukuran Lokasi: Quartile, Mean, dan Median Ukuran Variabilitas: Standar Deviasi Ukuran Bentuk: Skewness Adapun materi yang saya dapatkan tidak mencakup semua dari ukuran lokasi, variabilitas, dan bentuk. Ukuran Lokasi \u00b6 Quartile \u00b6 Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3. Quartile bisa dihitung menggunakan rumus sebagai berikut: $$ Q_{1}=x_{\\frac{1}{4}(n+1)} $$ Q_{2}=x_{\\frac{1}{2}(n+1)} Q_{2}=x_{\\frac{1}{2}(n+1)} Q_{3}=x_{\\frac{3}{4}(n+1)} Q_{3}=x_{\\frac{3}{4}(n+1)} Mean (rata-rata) \u00b6 Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : $$ \\overline{x}=\\frac{\\sum_{i=1}^{n} x_{i}}{N}=\\frac{x_{1}+x_{2}+x_{3}+\\ldots+x_{n}}{N} $$ Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Median \u00b6 Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut. Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: $$ M e=Q_{2}=\\left(\\frac{x_{n+1}}{2}\\right), \\text { jikanganjil } $$ M e=Q_{2}=\\left(\\frac{\\frac{x_{n}}{2} \\frac{x_{n+1}}{2}}{2}\\right), \\text { jikangenap } M e=Q_{2}=\\left(\\frac{\\frac{x_{n}}{2} \\frac{x_{n+1}}{2}}{2}\\right), \\text { jikangenap } Ukuran Variabilitas \u00b6 Standar Deviasi \u00b6 Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Standar Deviasi bisa didapat menggunakan rumus sebagai berikut: \\sigma=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_{1}-x\\right)^{2}}{n}} \\sigma=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_{1}-x\\right)^{2}}{n}} Ukuran Bentuk \u00b6 Skewness \u00b6 Skewness ( kemencengan ) adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). Skewness bisa dihitung menggunakan rumus sebagai berikut: \\text {Skewness}(S)=\\frac{1}{T \\sigma^{3}} \\sum_{i=1}^{n}\\left(r_{2}-\\mu\\right)^{3} \\text {Skewness}(S)=\\frac{1}{T \\sigma^{3}} \\sum_{i=1}^{n}\\left(r_{2}-\\mu\\right)^{3} Tugas dan Penerapan Statistik Deskriptif Menggunakan Python \u00b6 Pada perkuliahan saat itu saya mendapatkan tugas sebagai berikut: Alat dan Bahan \u00b6 pada tugas ini saya menggunakan 99 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam pembuatan program python nya nanti perlu disiapkan library python yang dapat di download secara gratis. dalam tugas ini saya menggunakan library python sebagai berikut: pandas, digunakan untuk data manajemen dan data analisis scipy, berisi kumpulan algoritme dan fungsi matematika. Langkah Pertama \u00b6 pada langkah ini kita menginisialisasi library yang telah di siapkan sebelumnya import pandas as pd from scipy import stats Langkah ke-Dua \u00b6 membuat variable untuk memuat data .csv yang telah disiapkan df = pd . read_csv ( \"datamining.csv\" , usecols = [ 0 , 1 , 2 , 3 , 4 ]) Langkah ke-Tiga \u00b6 Menginisialisasi kolom yang ada pada data .csv dan memasukannya ke dalam variabel clm , dan membuat penampungan list untuk hasil dari 6 metode clm = list ( df ) mean = [] std = [] skew = [] q1 = [] q2 = [] q3 = [] Langkah ke-Empat \u00b6 Melakukan perulangan dimana batas yang digunakan adalah banyak kolom, terus melakukan proses penggunaan metode dan menyimpannya ke dalam variabel Tmp , dan setelah itu data di append ke dalam variabel list yang sudah di buat sesuai dengan penggunaan metodenya for i in df : meanTmp = df [ i ] . mean () stdTmp = df [ i ] . std () skewTmp = df [ i ] . skew () q1Tmp = df [ i ] . quantile ( 0.25 ) q2Tmp = df [ i ] . quantile ( 0.5 ) q3Tmp = df [ i ] . quantile ( 0.75 ) mean . append ( meanTmp ) std . append ( stdTmp ) skew . append ( skewTmp ) q1 . append ( q1Tmp ) q2 . append ( q2Tmp ) q3 . append ( q3Tmp ) Langkah ke-Lima \u00b6 Membuat Variabel dengan tipe Dictionary dan mengisinya dengan data String sebagai penamaan kolom paling kiri dan row pertama sebagai header, dan datanya mengambil dari variabel list yang sudah terisi data dari looping tadi newData = { 'Statis' :[ 'Mean' , 'Std' , 'Q1' , 'Q2' , 'Q3' , 'Skewness' ], 'X1' :[ mean [ 0 ], std [ 0 ], q1 [ 0 ], q2 [ 0 ], q3 [ 0 ], skew [ 0 ]], 'X2' :[ mean [ 1 ], std [ 1 ], q1 [ 1 ], q2 [ 1 ], q3 [ 1 ], skew [ 1 ]], 'X3' :[ mean [ 2 ], std [ 2 ], q1 [ 2 ], q2 [ 2 ], q3 [ 2 ], skew [ 2 ]], 'X4' :[ mean [ 3 ], std [ 3 ], q1 [ 3 ], q2 [ 3 ], q3 [ 3 ], skew [ 3 ]], 'X5' :[ mean [ 4 ], std [ 4 ], q1 [ 4 ], q2 [ 4 ], q3 [ 4 ], skew [ 4 ]]} Langkah ke-Enam \u00b6 Membuat sebuah Dataframe baru dengan mengambil Variabel dari Dictionari yang sudah di buat tadi, dan setelah itu menampilkannya dfNew = pd . DataFrame ( newData ) print ( dfNew [[ 'Statis' , 'X1' , 'X2' , 'X3' , 'X4' , 'X5' ]]) Langkah ke-Tujuh \u00b6 Mengeksport hasil dari dataframenya kedalam .csv export_csv = dfNew . to_csv ( \"hasil.csv\" , index = False ) Source \u00b6 Apabila ingin melihat seluruh file source silahkan klik di sini Referensi \u00b6 http://fni-statistics.blogspot.com/2013/06/pengertian-statistik-deskriptif.html https://id.wikipedia.org/wiki/Statistika_deskriptif","title":"I. Statistik Deskriptif"},{"location":"semester-5/datamining/pt2/#pertemuan-ke-2","text":"Pada perkuliahan pertemuan kedua saya mendapatkan materi tentang Statistika Deskriptif . didalam perkuliahan ini saya diajarkan dengan berbagai metode - metode seperti mencari Mean, Median, Standart Deviasi, Skewness, dan Quartile.","title":"Pertemuan ke-2"},{"location":"semester-5/datamining/pt2/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"semester-5/datamining/pt2/#pengertian","text":"Statistika Deskriptif adalah metode - metode yang berkaitan dengan pengumpulan dan penyajian suatu gugus data sehingga memberikan informasi yang berguna. Statistika deskriptif hanya memberikan informasi mengenai data yang dipunyai dan sama sekali tidak menarik inferensia atau kesimpulan apapun tentang gugus induknya yang lebih besar. Contoh statistika deskriptif yang sering muncul adalah, tabel, diagram, grafik, dan besaran-besaran lain di majalah dan koran-koran. Dengan Statistika deskriptif, kumpulan data yang diperoleh akan tersaji dengan ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada. Informasi yang dapat diperoleh dari statistika deskriptif ini antara lain ukuran pemusatan data, ukuran penyebaran data, serta kecenderungan suatu gugus data. Statistika Deskriptif adalah statistika yang menggunakan data pada suatu kelompok untuk menjelaskan atau menarik suatu kesimpulan mengenai kelompok itu saja. Kelompok sendiri terdapat 3 ukuran yaitu Ukuran Lokasi, Ukuran Variabilitas, dan Ukuran Bentuk . Pada materi yang saya dapatkan sebagai berikut: Ukuran Lokasi: Quartile, Mean, dan Median Ukuran Variabilitas: Standar Deviasi Ukuran Bentuk: Skewness Adapun materi yang saya dapatkan tidak mencakup semua dari ukuran lokasi, variabilitas, dan bentuk.","title":"Pengertian"},{"location":"semester-5/datamining/pt2/#ukuran-lokasi","text":"","title":"Ukuran Lokasi"},{"location":"semester-5/datamining/pt2/#quartile","text":"Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3. Quartile bisa dihitung menggunakan rumus sebagai berikut: $$ Q_{1}=x_{\\frac{1}{4}(n+1)} $$ Q_{2}=x_{\\frac{1}{2}(n+1)} Q_{2}=x_{\\frac{1}{2}(n+1)} Q_{3}=x_{\\frac{3}{4}(n+1)} Q_{3}=x_{\\frac{3}{4}(n+1)}","title":"Quartile"},{"location":"semester-5/datamining/pt2/#mean-rata-rata","text":"Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : $$ \\overline{x}=\\frac{\\sum_{i=1}^{n} x_{i}}{N}=\\frac{x_{1}+x_{2}+x_{3}+\\ldots+x_{n}}{N} $$ Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data","title":"Mean (rata-rata)"},{"location":"semester-5/datamining/pt2/#median","text":"Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut. Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: $$ M e=Q_{2}=\\left(\\frac{x_{n+1}}{2}\\right), \\text { jikanganjil } $$ M e=Q_{2}=\\left(\\frac{\\frac{x_{n}}{2} \\frac{x_{n+1}}{2}}{2}\\right), \\text { jikangenap } M e=Q_{2}=\\left(\\frac{\\frac{x_{n}}{2} \\frac{x_{n+1}}{2}}{2}\\right), \\text { jikangenap }","title":"Median"},{"location":"semester-5/datamining/pt2/#ukuran-variabilitas","text":"","title":"Ukuran Variabilitas"},{"location":"semester-5/datamining/pt2/#standar-deviasi","text":"Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Standar Deviasi bisa didapat menggunakan rumus sebagai berikut: \\sigma=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_{1}-x\\right)^{2}}{n}} \\sigma=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_{1}-x\\right)^{2}}{n}}","title":"Standar Deviasi"},{"location":"semester-5/datamining/pt2/#ukuran-bentuk","text":"","title":"Ukuran Bentuk"},{"location":"semester-5/datamining/pt2/#skewness","text":"Skewness ( kemencengan ) adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). Skewness bisa dihitung menggunakan rumus sebagai berikut: \\text {Skewness}(S)=\\frac{1}{T \\sigma^{3}} \\sum_{i=1}^{n}\\left(r_{2}-\\mu\\right)^{3} \\text {Skewness}(S)=\\frac{1}{T \\sigma^{3}} \\sum_{i=1}^{n}\\left(r_{2}-\\mu\\right)^{3}","title":"Skewness"},{"location":"semester-5/datamining/pt2/#tugas-dan-penerapan-statistik-deskriptif-menggunakan-python","text":"Pada perkuliahan saat itu saya mendapatkan tugas sebagai berikut:","title":"Tugas dan Penerapan Statistik Deskriptif Menggunakan Python"},{"location":"semester-5/datamining/pt2/#alat-dan-bahan","text":"pada tugas ini saya menggunakan 99 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam pembuatan program python nya nanti perlu disiapkan library python yang dapat di download secara gratis. dalam tugas ini saya menggunakan library python sebagai berikut: pandas, digunakan untuk data manajemen dan data analisis scipy, berisi kumpulan algoritme dan fungsi matematika.","title":"Alat dan Bahan"},{"location":"semester-5/datamining/pt2/#langkah-pertama","text":"pada langkah ini kita menginisialisasi library yang telah di siapkan sebelumnya import pandas as pd from scipy import stats","title":"Langkah Pertama"},{"location":"semester-5/datamining/pt2/#langkah-ke-dua","text":"membuat variable untuk memuat data .csv yang telah disiapkan df = pd . read_csv ( \"datamining.csv\" , usecols = [ 0 , 1 , 2 , 3 , 4 ])","title":"Langkah ke-Dua"},{"location":"semester-5/datamining/pt2/#langkah-ke-tiga","text":"Menginisialisasi kolom yang ada pada data .csv dan memasukannya ke dalam variabel clm , dan membuat penampungan list untuk hasil dari 6 metode clm = list ( df ) mean = [] std = [] skew = [] q1 = [] q2 = [] q3 = []","title":"Langkah ke-Tiga"},{"location":"semester-5/datamining/pt2/#langkah-ke-empat","text":"Melakukan perulangan dimana batas yang digunakan adalah banyak kolom, terus melakukan proses penggunaan metode dan menyimpannya ke dalam variabel Tmp , dan setelah itu data di append ke dalam variabel list yang sudah di buat sesuai dengan penggunaan metodenya for i in df : meanTmp = df [ i ] . mean () stdTmp = df [ i ] . std () skewTmp = df [ i ] . skew () q1Tmp = df [ i ] . quantile ( 0.25 ) q2Tmp = df [ i ] . quantile ( 0.5 ) q3Tmp = df [ i ] . quantile ( 0.75 ) mean . append ( meanTmp ) std . append ( stdTmp ) skew . append ( skewTmp ) q1 . append ( q1Tmp ) q2 . append ( q2Tmp ) q3 . append ( q3Tmp )","title":"Langkah ke-Empat"},{"location":"semester-5/datamining/pt2/#langkah-ke-lima","text":"Membuat Variabel dengan tipe Dictionary dan mengisinya dengan data String sebagai penamaan kolom paling kiri dan row pertama sebagai header, dan datanya mengambil dari variabel list yang sudah terisi data dari looping tadi newData = { 'Statis' :[ 'Mean' , 'Std' , 'Q1' , 'Q2' , 'Q3' , 'Skewness' ], 'X1' :[ mean [ 0 ], std [ 0 ], q1 [ 0 ], q2 [ 0 ], q3 [ 0 ], skew [ 0 ]], 'X2' :[ mean [ 1 ], std [ 1 ], q1 [ 1 ], q2 [ 1 ], q3 [ 1 ], skew [ 1 ]], 'X3' :[ mean [ 2 ], std [ 2 ], q1 [ 2 ], q2 [ 2 ], q3 [ 2 ], skew [ 2 ]], 'X4' :[ mean [ 3 ], std [ 3 ], q1 [ 3 ], q2 [ 3 ], q3 [ 3 ], skew [ 3 ]], 'X5' :[ mean [ 4 ], std [ 4 ], q1 [ 4 ], q2 [ 4 ], q3 [ 4 ], skew [ 4 ]]}","title":"Langkah ke-Lima"},{"location":"semester-5/datamining/pt2/#langkah-ke-enam","text":"Membuat sebuah Dataframe baru dengan mengambil Variabel dari Dictionari yang sudah di buat tadi, dan setelah itu menampilkannya dfNew = pd . DataFrame ( newData ) print ( dfNew [[ 'Statis' , 'X1' , 'X2' , 'X3' , 'X4' , 'X5' ]])","title":"Langkah ke-Enam"},{"location":"semester-5/datamining/pt2/#langkah-ke-tujuh","text":"Mengeksport hasil dari dataframenya kedalam .csv export_csv = dfNew . to_csv ( \"hasil.csv\" , index = False )","title":"Langkah ke-Tujuh"},{"location":"semester-5/datamining/pt2/#source","text":"Apabila ingin melihat seluruh file source silahkan klik di sini","title":"Source"},{"location":"semester-5/datamining/pt2/#referensi","text":"http://fni-statistics.blogspot.com/2013/06/pengertian-statistik-deskriptif.html https://id.wikipedia.org/wiki/Statistika_deskriptif","title":"Referensi"},{"location":"semester-5/datamining/pt4/","text":"Pertemuan ke-4 \u00b6 Pada perkuliahan pertemuan keempat, saya mendapatkan materi tentang Missing Values . Dimana di dalam perkuliahan ini saya di ajarkan tentang mengisi sebuah value kosong dengan menggunakan berbagai metode seperti Median, Mean, dan KNN . Missing Values \u00b6 Missing Values adalah dimana informasi yang tidak tersedia (kosong) dalam sebuah object (data kasus). Missing values terjadi karena tidak di berikan, lupa memasukan, atau memang informasi tersebut tidak tersedia. Metode-metode untuk mengatasi Missing Values \u00b6 - Prosedur Berbasis Unit yang lengkap(Completely Record Units) \u00b6 Pada prosedur ini, analisisnya dilakukan terhadap data yang lengkap. Sedangkan data yang tidak lengkap akan diabaikan, atau bahkan dihilangkan. Dengan prasyarat data missing tidak terlalu besar, tapi prosedur ini tidak efisien jika presentase missing data (n2/2). 100 meningkat. - Prosedur Berbasis Imputasi \u00b6 Imputasi merupakan cara yang umum dan fleksibel. Biasanya orang yang mengedit data akan mengganti data kosong dengan nilai-nilai terdekat, atau memasukan nilai rata-rata yang telah di tentukan sebelumnya. Tugas dan Penerapan Impute Missing Values menggunakan KNN, Mean, dan Median \u00b6 Alat dan Bahan \u00b6 Pada tugas ini saya menggunakan data yang bernama hepatitis.csv dan untuk beberapa library python yang perlu dipersiapkan. pandas, digunakan untuk data manajemen dan analisis data numpy, digunakan untuk mengelola array dan array multi dimensi missingpy, digunakan untuk mengelola missing value dengan metode knn scikit-learn, digunakan untuk mengelola data science yang super lengkap Data hepatitis Menggunakan KNN \u00b6 Langkah Pertama \u00b6 Pada langkah ini kita menginisialisasi library yang telah di siapkan sebelumnya dan setelah itu memuat data csv yang telah di siapkan import pandas as pd import numpy from missingpy import KNNImputer df = pd . read_csv ( \"hepatitis.csv\" ) df Langkah ke-Dua \u00b6 Pada langkah selanjutnya kita akan melakukan pengoperasian pada data. yang pertama melakukan penandaan values yang kosong dengan memberi NaN, setelah itu membuat variabel imputer untuk menampung data hasil proses imputasi data yang kosong dengan metode knn dan setelah itu menset data hasil proses tadi kedalam variabel newData dan setelah itu menyimpan kedalam dataFrame dfNew dan menampilkannya nan = numpy . nan imputer = KNNImputer ( n_neighbors = 2 , weights = \"uniform\" ) newData = imputer . fit_transform ( df ) dfNew = pd . DataFrame ( newData ) dfNew Langkah ke-Tiga \u00b6 Langkah terakhir adalah mengeksport hasil dari dataframenya kedalam .csv export_csv = dfNew . to_csv ( \"hepatitis_knn.csv\" , index = False ) Menggunakan Mean \u00b6 Langkah Pertama \u00b6 Pada langkah pertama kita menginisialisasi library yang sudah kita siapkan tadi, dan setelah itu memuat data csvnya import pandas as pd import numpy as np from sklearn.impute import SimpleImputer df = pd . read_csv ( \"hepatitis.csv\" ) df Langkah ke-Dua \u00b6 Pada langkah selanjutnya kita membuat sebuah variable imp dimana memilihi sebuah metode yang digunakan dalam penggunaan fungsi scikit-learn yaitu simple imputer metode mean. Dan setelah itu membuat sebuah variabel newData yang dimana untuk menyimpan hasil proses berisi transformasi dari fungsi imp tadi. Setelah itu membuat dataFrame baru dan menampilkannya. imp = SimpleImputer ( strategy = \"mean\" ) newData = imp . fit_transform ( df ) dfNew = pd . DataFrame ( newData ) dfNew Langkah ke-Tiga \u00b6 Langkah terakhir adalah mengeksport hasil dari dataframenya kedalam .csv export_csv = dfNew . to_csv ( \"hepatitis_mean.csv\" ) Menggunakan Median \u00b6 Langkah Pertama \u00b6 Pada langkah pertama kita menginisialisasi library yang sudah kita siapkan tadi, dan setelah itu memuat data csvnya import pandas as pd import numpy as np from sklearn.impute import SimpleImputer df = pd . read_csv ( \"hepatitis.csv\" ) df Langkah ke-Dua \u00b6 Pada langkah selanjutnya kita membuat sebuah variable imp dimana memilihi sebuah metode yang digunakan dalam penggunaan fungsi scikit-learn yaitu simple imputer metode median. Dan setelah itu membuat sebuah variabel newData yang dimana untuk menyimpan hasil proses berisi transformasi dari fungsi imp tadi. Setelah itu membuat dataFrame baru dan menampilkannya. imp = SimpleImputer ( strategy = \"median\" ) newData = imp . fit_transform ( df ) dfNew = pd . DataFrame ( newData ) dfNew Langkah ke-Tiga \u00b6 Langkah terakhir adalah mengeksport hasil dari dataframenya kedalam .csv export_csv = dfNew . to_csv ( \"hepatitis_median.csv\" ) Source \u00b6 Apabila ingin melihat seluruh file sourcenya klik di sini Referensi \u00b6 https://archive.ics.uci.edu/ml/datasets/hepatitis","title":"II. Impute Missing Values with KNN"},{"location":"semester-5/datamining/pt4/#pertemuan-ke-4","text":"Pada perkuliahan pertemuan keempat, saya mendapatkan materi tentang Missing Values . Dimana di dalam perkuliahan ini saya di ajarkan tentang mengisi sebuah value kosong dengan menggunakan berbagai metode seperti Median, Mean, dan KNN .","title":"Pertemuan ke-4"},{"location":"semester-5/datamining/pt4/#missing-values","text":"Missing Values adalah dimana informasi yang tidak tersedia (kosong) dalam sebuah object (data kasus). Missing values terjadi karena tidak di berikan, lupa memasukan, atau memang informasi tersebut tidak tersedia.","title":"Missing Values"},{"location":"semester-5/datamining/pt4/#metode-metode-untuk-mengatasi-missing-values","text":"","title":"Metode-metode untuk mengatasi Missing Values"},{"location":"semester-5/datamining/pt4/#-prosedur-berbasis-unit-yang-lengkapcompletely-record-units","text":"Pada prosedur ini, analisisnya dilakukan terhadap data yang lengkap. Sedangkan data yang tidak lengkap akan diabaikan, atau bahkan dihilangkan. Dengan prasyarat data missing tidak terlalu besar, tapi prosedur ini tidak efisien jika presentase missing data (n2/2). 100 meningkat.","title":"- Prosedur Berbasis Unit yang lengkap(Completely Record Units)"},{"location":"semester-5/datamining/pt4/#-prosedur-berbasis-imputasi","text":"Imputasi merupakan cara yang umum dan fleksibel. Biasanya orang yang mengedit data akan mengganti data kosong dengan nilai-nilai terdekat, atau memasukan nilai rata-rata yang telah di tentukan sebelumnya.","title":"- Prosedur Berbasis Imputasi"},{"location":"semester-5/datamining/pt4/#tugas-dan-penerapan-impute-missing-values-menggunakan-knn-mean-dan-median","text":"","title":"Tugas dan Penerapan Impute Missing Values menggunakan KNN, Mean, dan Median"},{"location":"semester-5/datamining/pt4/#alat-dan-bahan","text":"Pada tugas ini saya menggunakan data yang bernama hepatitis.csv dan untuk beberapa library python yang perlu dipersiapkan. pandas, digunakan untuk data manajemen dan analisis data numpy, digunakan untuk mengelola array dan array multi dimensi missingpy, digunakan untuk mengelola missing value dengan metode knn scikit-learn, digunakan untuk mengelola data science yang super lengkap Data hepatitis","title":"Alat dan Bahan"},{"location":"semester-5/datamining/pt4/#menggunakan-knn","text":"","title":"Menggunakan KNN"},{"location":"semester-5/datamining/pt4/#langkah-pertama","text":"Pada langkah ini kita menginisialisasi library yang telah di siapkan sebelumnya dan setelah itu memuat data csv yang telah di siapkan import pandas as pd import numpy from missingpy import KNNImputer df = pd . read_csv ( \"hepatitis.csv\" ) df","title":"Langkah Pertama"},{"location":"semester-5/datamining/pt4/#langkah-ke-dua","text":"Pada langkah selanjutnya kita akan melakukan pengoperasian pada data. yang pertama melakukan penandaan values yang kosong dengan memberi NaN, setelah itu membuat variabel imputer untuk menampung data hasil proses imputasi data yang kosong dengan metode knn dan setelah itu menset data hasil proses tadi kedalam variabel newData dan setelah itu menyimpan kedalam dataFrame dfNew dan menampilkannya nan = numpy . nan imputer = KNNImputer ( n_neighbors = 2 , weights = \"uniform\" ) newData = imputer . fit_transform ( df ) dfNew = pd . DataFrame ( newData ) dfNew","title":"Langkah ke-Dua"},{"location":"semester-5/datamining/pt4/#langkah-ke-tiga","text":"Langkah terakhir adalah mengeksport hasil dari dataframenya kedalam .csv export_csv = dfNew . to_csv ( \"hepatitis_knn.csv\" , index = False )","title":"Langkah ke-Tiga"},{"location":"semester-5/datamining/pt4/#menggunakan-mean","text":"","title":"Menggunakan Mean"},{"location":"semester-5/datamining/pt4/#langkah-pertama_1","text":"Pada langkah pertama kita menginisialisasi library yang sudah kita siapkan tadi, dan setelah itu memuat data csvnya import pandas as pd import numpy as np from sklearn.impute import SimpleImputer df = pd . read_csv ( \"hepatitis.csv\" ) df","title":"Langkah Pertama"},{"location":"semester-5/datamining/pt4/#langkah-ke-dua_1","text":"Pada langkah selanjutnya kita membuat sebuah variable imp dimana memilihi sebuah metode yang digunakan dalam penggunaan fungsi scikit-learn yaitu simple imputer metode mean. Dan setelah itu membuat sebuah variabel newData yang dimana untuk menyimpan hasil proses berisi transformasi dari fungsi imp tadi. Setelah itu membuat dataFrame baru dan menampilkannya. imp = SimpleImputer ( strategy = \"mean\" ) newData = imp . fit_transform ( df ) dfNew = pd . DataFrame ( newData ) dfNew","title":"Langkah ke-Dua"},{"location":"semester-5/datamining/pt4/#langkah-ke-tiga_1","text":"Langkah terakhir adalah mengeksport hasil dari dataframenya kedalam .csv export_csv = dfNew . to_csv ( \"hepatitis_mean.csv\" )","title":"Langkah ke-Tiga"},{"location":"semester-5/datamining/pt4/#menggunakan-median","text":"","title":"Menggunakan Median"},{"location":"semester-5/datamining/pt4/#langkah-pertama_2","text":"Pada langkah pertama kita menginisialisasi library yang sudah kita siapkan tadi, dan setelah itu memuat data csvnya import pandas as pd import numpy as np from sklearn.impute import SimpleImputer df = pd . read_csv ( \"hepatitis.csv\" ) df","title":"Langkah Pertama"},{"location":"semester-5/datamining/pt4/#langkah-ke-dua_2","text":"Pada langkah selanjutnya kita membuat sebuah variable imp dimana memilihi sebuah metode yang digunakan dalam penggunaan fungsi scikit-learn yaitu simple imputer metode median. Dan setelah itu membuat sebuah variabel newData yang dimana untuk menyimpan hasil proses berisi transformasi dari fungsi imp tadi. Setelah itu membuat dataFrame baru dan menampilkannya. imp = SimpleImputer ( strategy = \"median\" ) newData = imp . fit_transform ( df ) dfNew = pd . DataFrame ( newData ) dfNew","title":"Langkah ke-Dua"},{"location":"semester-5/datamining/pt4/#langkah-ke-tiga_2","text":"Langkah terakhir adalah mengeksport hasil dari dataframenya kedalam .csv export_csv = dfNew . to_csv ( \"hepatitis_median.csv\" )","title":"Langkah ke-Tiga"},{"location":"semester-5/datamining/pt4/#source","text":"Apabila ingin melihat seluruh file sourcenya klik di sini","title":"Source"},{"location":"semester-5/datamining/pt4/#referensi","text":"https://archive.ics.uci.edu/ml/datasets/hepatitis","title":"Referensi"},{"location":"semester-8/","text":"Mata Kuliah Semester 8 \u00b6","title":"Matakuliah Semester 8"},{"location":"semester-8/#mata-kuliah-semester-8","text":"","title":"Mata Kuliah Semester 8"},{"location":"semester-8/webmining/","text":"Web Mining \u00b6 Sebelum masuk ke dalam web mining alangkah baiknya memahami dulu pengertian tentang data mining. Pengertian Datamining \u00b6 Datamining disebut juga sebagai Penambangan Data. Penambangan Data sendiri adalah proses ekstrasi pola yang menarik dari data dalam jumlah besar yang melibatkan penggunaan metode machine learning , statistik , dan sistem basis data . Tugas penambangan data aktual adalah analisis semi-otomatis atau otomatis dari sejumlah besar data untuk mengekstraksi pola yang sebelumnya tidak diketahui, contoh pola - pola menarik seperti : kelompok catatan data (cluster analysis) , catatan tidak biasa (anomali detection) , dan dependencies (asosiasi aturan penambangan , penambangan pola berurutan) . ini biasanya melibatkan penggunaan teknik basis data seperti indeks spasial . Pola yang disajikan haruslah mudah di pahami, berlaku untuk data yang akan di prediksi dengan derajat kepastian tertentu, berguna, dan baru. Proses Pencarian Pola (Patern) \u00b6 Penambangan data adalah salah satu bagian dari proses pencarian pola. Berikut ini urutan proses pencarian pola: Pembersihan Data : yaitu menghapus data pengganggu (noise) dan mengisi data yang hilang. Integrasi Data : yaitu menggabungkan berbagai sumber data. Pemilihan Data : yaitu memilih data yang relevan. Transformasi Data : yaitu mentransformasikan data ke dalam format untuk di proses dalam penambangan data. Penambangan Data : yaitu menerapkan metode cerdas untuk ekstraksi pola. Evaluasi Pola : yaitu mengenali pola - pola yang menarik saja. Penyajian Pola : yaitu memvisualisasikan pola ke pengguna. Teknik Penambangan Data \u00b6 Pada dasarnya penambangan data di bedakan menjadi dua fungsionalitas, yaitu deskripsi dan prediksi. Berikut beberapa fungsionalitas penambangan data yang sering digunakan: Karakteristik dan Diskriminasi : menggeneralisasi, merangkum, dan mengkontraskan karakteristik data Penambangan pola berulang : pencarian data dari suatu transaksi yang berulang Klasifikasi : membangun suatu model yang bisa mengkasifikasikan suatu objek berdasarkan atribut-atributnya. Prediksi : memprediksi nilai yang tidak di ketahui atau nilai yang hilang, menggunakan model dari klasifikasi. Penggugusan / Cluster Analysis : pengkelompokan kumpulan objek data berdasarkan kemiripannya. Analisis Outlier : proses mengenali noise dan pengecualian dalam data. Analisis Trend dan Evolusi : meliputi analisis regresi, penambangan pola sekuensial, analisis periode, dan analisis berbasis kemiripan. Pengertian Web Mining \u00b6 Web mining merupakan penerapan teknik data mining terhadap web dengan tujuan untuk memperoleh pengetahuan dan informasi lebih lanjut dari halaman web. Web mining dapat dikategorikan ke dalam tiga ruang lingkup yang berbeda yaitu web content mining, web structure mining dan web usage mining . Web Content Mining \u00b6 Web contentent mining mengacu pada ekstraksi informasi yang berguna dari halaman web. Dokumen dapat diekstrak dalam beberapa format terbaca-mesin sehingga teknik otomatis dapat menghasilkan beberapa informasi tentang halaman web. Web crawler digunakan untuk membaca isi sebuah situs web secara otomatis. informasi yang dikumpulkan dapat meliputi karakteristik dokumen mirip dengan apa yang digunakan dalam text mining, tetapi bisa termasuk konsep tambahan, seperti hirarki dokumen. Web Content Mining juga dapat digunakan untuk meningkatkan hasil yang dihasilkan oleh mesin pencari. Web Structure Mining \u00b6 Web structure mining adalah proses penggalian informasi yang berguna dari link embedded dalam dokumen web. Digunakan untuk mengidentifikasi otoritatif halaman dan hub, yang merupakan landasan dari algoritma page-rank kontemporer yang penting bagi mesin pencari populer seperti Google dan Yahoo!. Analisis link sangat penting dalam memahami hubungan timbal balik antara sejumlah besar halaman web, yang mengarah ke pemahaman yang lebih baik dari komunitas web tertentu, klan, atau klik. Web Usage Mining \u00b6 Web usage mining adalah pengambilan informasi yang berguna dari data yang dihasilkan melalui kunjungan halaman web dan transaksi. Masand et al. (2002) menyatakan bahwa setidaknya tiga jenis data yang dihasilkan melalui kunjungan halaman web: Secara otomatis data yang tersimpan dalam server access log, referrer log, agent log, dan cookie client-side Profil Pengguna Metadata, seperti atribut halaman, atribut konten, dan data penggunaan. Referensi https://en.wikipedia.org/wiki/Data_mining https://id.wikipedia.org/wiki/Penggalian_data https://voetstappen.wordpress.com/2011/05/20/text-dan-we-mining/","title":"Apa itu Webmining?"},{"location":"semester-8/webmining/#web-mining","text":"Sebelum masuk ke dalam web mining alangkah baiknya memahami dulu pengertian tentang data mining.","title":"Web Mining"},{"location":"semester-8/webmining/#pengertian-datamining","text":"Datamining disebut juga sebagai Penambangan Data. Penambangan Data sendiri adalah proses ekstrasi pola yang menarik dari data dalam jumlah besar yang melibatkan penggunaan metode machine learning , statistik , dan sistem basis data . Tugas penambangan data aktual adalah analisis semi-otomatis atau otomatis dari sejumlah besar data untuk mengekstraksi pola yang sebelumnya tidak diketahui, contoh pola - pola menarik seperti : kelompok catatan data (cluster analysis) , catatan tidak biasa (anomali detection) , dan dependencies (asosiasi aturan penambangan , penambangan pola berurutan) . ini biasanya melibatkan penggunaan teknik basis data seperti indeks spasial . Pola yang disajikan haruslah mudah di pahami, berlaku untuk data yang akan di prediksi dengan derajat kepastian tertentu, berguna, dan baru.","title":"Pengertian Datamining"},{"location":"semester-8/webmining/#proses-pencarian-pola-patern","text":"Penambangan data adalah salah satu bagian dari proses pencarian pola. Berikut ini urutan proses pencarian pola: Pembersihan Data : yaitu menghapus data pengganggu (noise) dan mengisi data yang hilang. Integrasi Data : yaitu menggabungkan berbagai sumber data. Pemilihan Data : yaitu memilih data yang relevan. Transformasi Data : yaitu mentransformasikan data ke dalam format untuk di proses dalam penambangan data. Penambangan Data : yaitu menerapkan metode cerdas untuk ekstraksi pola. Evaluasi Pola : yaitu mengenali pola - pola yang menarik saja. Penyajian Pola : yaitu memvisualisasikan pola ke pengguna.","title":"Proses Pencarian Pola (Patern)"},{"location":"semester-8/webmining/#teknik-penambangan-data","text":"Pada dasarnya penambangan data di bedakan menjadi dua fungsionalitas, yaitu deskripsi dan prediksi. Berikut beberapa fungsionalitas penambangan data yang sering digunakan: Karakteristik dan Diskriminasi : menggeneralisasi, merangkum, dan mengkontraskan karakteristik data Penambangan pola berulang : pencarian data dari suatu transaksi yang berulang Klasifikasi : membangun suatu model yang bisa mengkasifikasikan suatu objek berdasarkan atribut-atributnya. Prediksi : memprediksi nilai yang tidak di ketahui atau nilai yang hilang, menggunakan model dari klasifikasi. Penggugusan / Cluster Analysis : pengkelompokan kumpulan objek data berdasarkan kemiripannya. Analisis Outlier : proses mengenali noise dan pengecualian dalam data. Analisis Trend dan Evolusi : meliputi analisis regresi, penambangan pola sekuensial, analisis periode, dan analisis berbasis kemiripan.","title":"Teknik Penambangan Data"},{"location":"semester-8/webmining/#pengertian-web-mining","text":"Web mining merupakan penerapan teknik data mining terhadap web dengan tujuan untuk memperoleh pengetahuan dan informasi lebih lanjut dari halaman web. Web mining dapat dikategorikan ke dalam tiga ruang lingkup yang berbeda yaitu web content mining, web structure mining dan web usage mining .","title":"Pengertian Web Mining"},{"location":"semester-8/webmining/#web-content-mining","text":"Web contentent mining mengacu pada ekstraksi informasi yang berguna dari halaman web. Dokumen dapat diekstrak dalam beberapa format terbaca-mesin sehingga teknik otomatis dapat menghasilkan beberapa informasi tentang halaman web. Web crawler digunakan untuk membaca isi sebuah situs web secara otomatis. informasi yang dikumpulkan dapat meliputi karakteristik dokumen mirip dengan apa yang digunakan dalam text mining, tetapi bisa termasuk konsep tambahan, seperti hirarki dokumen. Web Content Mining juga dapat digunakan untuk meningkatkan hasil yang dihasilkan oleh mesin pencari.","title":"Web Content Mining"},{"location":"semester-8/webmining/#web-structure-mining","text":"Web structure mining adalah proses penggalian informasi yang berguna dari link embedded dalam dokumen web. Digunakan untuk mengidentifikasi otoritatif halaman dan hub, yang merupakan landasan dari algoritma page-rank kontemporer yang penting bagi mesin pencari populer seperti Google dan Yahoo!. Analisis link sangat penting dalam memahami hubungan timbal balik antara sejumlah besar halaman web, yang mengarah ke pemahaman yang lebih baik dari komunitas web tertentu, klan, atau klik.","title":"Web Structure Mining"},{"location":"semester-8/webmining/#web-usage-mining","text":"Web usage mining adalah pengambilan informasi yang berguna dari data yang dihasilkan melalui kunjungan halaman web dan transaksi. Masand et al. (2002) menyatakan bahwa setidaknya tiga jenis data yang dihasilkan melalui kunjungan halaman web: Secara otomatis data yang tersimpan dalam server access log, referrer log, agent log, dan cookie client-side Profil Pengguna Metadata, seperti atribut halaman, atribut konten, dan data penggunaan. Referensi https://en.wikipedia.org/wiki/Data_mining https://id.wikipedia.org/wiki/Penggalian_data https://voetstappen.wordpress.com/2011/05/20/text-dan-we-mining/","title":"Web Usage Mining"},{"location":"semester-8/webmining/mt1/","text":"Ringkasan Materi ke-1 \u00b6 Pada perkuliahan pertemuan kedua saya mendapatkan materi tentang Web Crawler & Web Scraper . didalam perkuliahan ini saya diajarkan tentang bagaimana cara pengambilan data text dari sebuah website. Web Crawler \u00b6 Web crawler adalah proses mengunjungi situs web, membaca halaman web, menemukan informasi dari suatu web dan mengindeks semua kata dalam sebuah dokumen. Web crawler bisa juga disebut sebagai Bot untuk menjelajah sebuah website secara otomatis dengan link yang saling bertautan. Web Scraper \u00b6 Web scraper, web harvesting, web data extraction merupakan kegiatan yang dilakukan untuk mengambil data tertentu secara semi-terstruktur dari sebuah halaman website. Halaman tersebut umumnya dibangun menggunakan bahasa markup seperti HTML atau XHTML , proses akan menganalisis dokumen sebelum memulai mengambil data. Biasanya teknik scraping diimplementasikan pada sebuah bot (crawler) agar bisa membuat proses yang harusnya dilakukan secara manual menjadi otomatis. Ketika kita menjumpai sebuah situs yang membatasi kuota API (Application Programming Interface) atau bahkan tidak menyediakan sama sekali, maka perayapan web akan sangat dibutuhkan sebagai langkah pengambilan data. Tugas pada perkuliahan saat itu saya mendapatkan tugas sebagai berikut: \u00b6 Scraper sebuah website dengan python dengan menggunakan library scrapy. Alat dan Bahan \u00b6 pada tugas ini saya menggunakan alamat website gearbest untuk target mengambil data produk flash sale dan untuk mempermudah dalam pembuatan program python nya nanti perlu disiapkan library python yang dapat di download secara gratis. dalam tugas ini saya menggunakan library python sebagai berikut: scrapy, library python yang dapat dengan mudah melakukan scrape data website Langkah Pertama \u00b6 pada langkah ini kita membuat sebuah project scrapy dengan cara 1. Membuka cmd 2. ketik : scrapy startproject \"nama project\" Langkah ke-Dua \u00b6 Membuat file python di folder scrapy project yang sudah kita buat tadi di dalam folder spiders Langkah ke-Tiga \u00b6 Melakukan import library scrapy import scrapy Langkah ke-Empat \u00b6 Membuat sebuah class dan menigisi sebuah parameter dari scrapy spider class GearbestScrapySpider ( scrapy . Spider ): Langkah ke-Lima \u00b6 Membuat Variabel name dengan isi berupa data string yang bertujuan untuk melakukan pemanggilan waktu di cmd name = 'flashsale_scrapy' Langkah ke-Enam \u00b6 Membuat sebuah fungsi untuk melakukan start request dimana didalam fungsi tersebut digunakan untuk inisialisasi url dan callback untuk pengembalian data def start_requests ( self ): urlBarang = \"https://www.gearbest.com/flash-sale.html\" yield scrapy . Request ( url = urlBarang , callback = self . parse ) Langkah ke-Tujuh \u00b6 Membuat sebuah fungsi untuk melakukan parse data dimana didalam fungsi tersebut digunakan untuk menyimpan data file yang sudah di parse (path markup html yang sudah ditentukan) def parse ( self , response ): newCsv = open ( 'nama_produk_flashsale.csv' , 'a' ) for j in response . xpath ( '//div[contains(@class, \"goodsItem_title\")]/a' ): newCsv . write ( j . xpath ( 'text()' ) . extract_first ()) newCsv . close () Langkah ke-Delapan \u00b6 Melakukan run spider yang sudah kita buat tadi 1. buka cmd 2. arahkan path ke folder project 3. ketik scrapy crawl flashsale_scrapy Source \u00b6 Apabila ingin melihat seluruh file source silahkan klik di sini Referensi \u00b6 https://id.wikipedia.org/wiki/Web_scraping https://www.dqlab.id/teknik-pengumpulan-data-sekunder-dengan-web-crawling","title":"I. Web Crawler & Web Scraper"},{"location":"semester-8/webmining/mt1/#ringkasan-materi-ke-1","text":"Pada perkuliahan pertemuan kedua saya mendapatkan materi tentang Web Crawler & Web Scraper . didalam perkuliahan ini saya diajarkan tentang bagaimana cara pengambilan data text dari sebuah website.","title":"Ringkasan Materi ke-1"},{"location":"semester-8/webmining/mt1/#web-crawler","text":"Web crawler adalah proses mengunjungi situs web, membaca halaman web, menemukan informasi dari suatu web dan mengindeks semua kata dalam sebuah dokumen. Web crawler bisa juga disebut sebagai Bot untuk menjelajah sebuah website secara otomatis dengan link yang saling bertautan.","title":"Web Crawler"},{"location":"semester-8/webmining/mt1/#web-scraper","text":"Web scraper, web harvesting, web data extraction merupakan kegiatan yang dilakukan untuk mengambil data tertentu secara semi-terstruktur dari sebuah halaman website. Halaman tersebut umumnya dibangun menggunakan bahasa markup seperti HTML atau XHTML , proses akan menganalisis dokumen sebelum memulai mengambil data. Biasanya teknik scraping diimplementasikan pada sebuah bot (crawler) agar bisa membuat proses yang harusnya dilakukan secara manual menjadi otomatis. Ketika kita menjumpai sebuah situs yang membatasi kuota API (Application Programming Interface) atau bahkan tidak menyediakan sama sekali, maka perayapan web akan sangat dibutuhkan sebagai langkah pengambilan data.","title":"Web Scraper"},{"location":"semester-8/webmining/mt1/#tugas-pada-perkuliahan-saat-itu-saya-mendapatkan-tugas-sebagai-berikut","text":"Scraper sebuah website dengan python dengan menggunakan library scrapy.","title":"Tugas pada perkuliahan saat itu saya mendapatkan tugas sebagai berikut:"},{"location":"semester-8/webmining/mt1/#alat-dan-bahan","text":"pada tugas ini saya menggunakan alamat website gearbest untuk target mengambil data produk flash sale dan untuk mempermudah dalam pembuatan program python nya nanti perlu disiapkan library python yang dapat di download secara gratis. dalam tugas ini saya menggunakan library python sebagai berikut: scrapy, library python yang dapat dengan mudah melakukan scrape data website","title":"Alat dan Bahan"},{"location":"semester-8/webmining/mt1/#langkah-pertama","text":"pada langkah ini kita membuat sebuah project scrapy dengan cara 1. Membuka cmd 2. ketik : scrapy startproject \"nama project\"","title":"Langkah Pertama"},{"location":"semester-8/webmining/mt1/#langkah-ke-dua","text":"Membuat file python di folder scrapy project yang sudah kita buat tadi di dalam folder spiders","title":"Langkah ke-Dua"},{"location":"semester-8/webmining/mt1/#langkah-ke-tiga","text":"Melakukan import library scrapy import scrapy","title":"Langkah ke-Tiga"},{"location":"semester-8/webmining/mt1/#langkah-ke-empat","text":"Membuat sebuah class dan menigisi sebuah parameter dari scrapy spider class GearbestScrapySpider ( scrapy . Spider ):","title":"Langkah ke-Empat"},{"location":"semester-8/webmining/mt1/#langkah-ke-lima","text":"Membuat Variabel name dengan isi berupa data string yang bertujuan untuk melakukan pemanggilan waktu di cmd name = 'flashsale_scrapy'","title":"Langkah ke-Lima"},{"location":"semester-8/webmining/mt1/#langkah-ke-enam","text":"Membuat sebuah fungsi untuk melakukan start request dimana didalam fungsi tersebut digunakan untuk inisialisasi url dan callback untuk pengembalian data def start_requests ( self ): urlBarang = \"https://www.gearbest.com/flash-sale.html\" yield scrapy . Request ( url = urlBarang , callback = self . parse )","title":"Langkah ke-Enam"},{"location":"semester-8/webmining/mt1/#langkah-ke-tujuh","text":"Membuat sebuah fungsi untuk melakukan parse data dimana didalam fungsi tersebut digunakan untuk menyimpan data file yang sudah di parse (path markup html yang sudah ditentukan) def parse ( self , response ): newCsv = open ( 'nama_produk_flashsale.csv' , 'a' ) for j in response . xpath ( '//div[contains(@class, \"goodsItem_title\")]/a' ): newCsv . write ( j . xpath ( 'text()' ) . extract_first ()) newCsv . close ()","title":"Langkah ke-Tujuh"},{"location":"semester-8/webmining/mt1/#langkah-ke-delapan","text":"Melakukan run spider yang sudah kita buat tadi 1. buka cmd 2. arahkan path ke folder project 3. ketik scrapy crawl flashsale_scrapy","title":"Langkah ke-Delapan"},{"location":"semester-8/webmining/mt1/#source","text":"Apabila ingin melihat seluruh file source silahkan klik di sini","title":"Source"},{"location":"semester-8/webmining/mt1/#referensi","text":"https://id.wikipedia.org/wiki/Web_scraping https://www.dqlab.id/teknik-pengumpulan-data-sekunder-dengan-web-crawling","title":"Referensi"},{"location":"semester-8/webmining/mt2/","text":"Ringkasan Materi ke-2 \u00b6 Pada perkuliahan pertemuan keempat saya mendapatkan materi tentang Text Preprocessing . didalam perkuliahan ini saya diajarkan tentang bagaimana cara membersihkan sebuah data text agar menjadi data yang dapat diolah lebih lanjut. Text Preprocessing \u00b6 Text preprocessing merupakan tahapan awal yang sangat penting sebelum melakukan proses ketahap lebih lanjut seperti sentimen analisis, topic modeling, dll. Data yang digunakan dalam proses mining tidak selamanya dalam kondisi yang ideal untuk diproses. Terkadang pada data tersebut terdapat berbagai permasalahan yang dapat menggangu hasil dari proses mining itu sendiri seperi diantaranya adalah missing value, data redundant, outliers, ataupun format data yang tidak sesuai dengan sistem. Oleh karenanya untuk mengatasi permasalahan tersebut dibutuhkan tahap Preprocessing. Preprocessing merupakan salah satu tahapan menghilangkan permasalahan-permasalahan yang dapat mengganggu hasil daripada proses data. 1. Case Folding \u00b6 Case folding adalah salah satu bentuk teks preprocessing yang paling sederhana. Dimana didalam case folding bertujuan untuk merubah semua huruf dalam dokumen menjadi huruf kecil, serta dapat juga digunakan untuk menghapus tanda baca dan angka yang tidak dibutuhkan pada saat proses case folding. 2. Tokenizing \u00b6 Tokenizing adalah proses pemisahan teks kalimat menjadi potongan-potongan kata yang disebut sebagai token untuk kemudian dapat di proses lanjut dengan stopwords. 3. Stopword \u00b6 Stopword adalah tahap mengambil kata-kata penting dari hasil token dengan menggunakan algoritma spotlist (membuang kata kurang penting). Stopword juga dapat menghapus kata umum yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna seperti (yang, dan, di, dari. dll). 4. Stemming \u00b6 Stemming adalah proses menghilangkan infleksi kata menjadi kata dasarnya, misalnya banyak penggunaan penambahan kalimat. Misalnya kata \u201cmendengarkan\u201d, \u201cdengarkan\u201d, \u201cdidengarkan\u201d akan ditransformasi menjadi kata \u201cdengar\u201d. Tugas pada perkuliahan saat itu saya mendapatkan tugas sebagai berikut: \u00b6 Melakukan text preprocessing setelah melakukan tugas scraping website berita. Alat dan Bahan \u00b6 pada tugas ini saya menggunakan alamat website suarasurabaya untuk target mengambil data berita politik dan untuk mempermudah dalam pembuatan program python nya nanti perlu disiapkan library python yang dapat di download secara gratis. dalam tugas ini saya menggunakan library python sebagai berikut: re, library python yang berguna untuk penggunaan regular expression string, library python yang berguna untuk manipulasi tipe data string nltk.tokenize word_tokenize, library python yang berguna untuk melakukan tokenize setiap kata nltk.corpus stopwords, library python yang berguna untuk melakukan penghapusan kata yang tidak penting sesuai kata wordlist nltk.corpus words, library python berupa wordlist yang akan digunakan untuk stopwords sastrawi.stemmer, library python berupa wordlist bahasa indonesia yg digunakan pada saat proses stemm 1. Case Folding \u00b6 Disini saya akan menggunakan library re, dan string untuk proses manipulasi semua teks import re import string #----------- Step1 Case Folding merubah atau menghilangkan karakter yang tidak perlu ------------ text . lower () #merubah semua huruf ke huruf kecil text = re . sub ( r \"http\\S+|www\\S+|https\\S+\" , '' , text , flags = re . MULTILINE ) #menghapus urls text = re . sub ( r '\\d+' , '' , text ) #menghapus berdasarkan regular expression (angka 0 - 9) text = re . sub ( r '@[\\w_]+|#+[\\w_]+' , '' , text ) #menghapus user @ references and '#' from tag text = text . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) #menghapus tanda baca ,.dsb text = text . strip () #menghapus karakter kosong / double space 2. Tokenizing \u00b6 Disini saya akan menggunakan library nltk word_tokenize, yang berguna untuk melakukan tokenize menjadi tiap kata from nltk.tokenize import word_tokenize #----------- Step2 Tokenizing data / memisahkan setiap kata ------------ tweet_tokens = word_tokenize ( text ) #tokenizing kata dari data text dengan menggunakan nltk tokenizing 3. Stopword \u00b6 Pada tahap ini akan dilakukan proses stopword / filtering kata yang tidak penting from nltk.corpus import stopwords from nltk.corpus import words stop_words = set ( stopwords . words ( 'indonesian' )) #mengatur seleksi text dalam bahasa indonesia menggunakan corpus corpus_words = set ( words . words ()) #mengatur seleksi text dalam bahasa inggris menggunakan corpus #----------- Step3 Filtering stopword / penghapusan kata yang tidak penting ------------ filtered_words = [ w for w in tweet_tokens if not w in stop_words ] #melakukan perulangan penghapusan text berdasarkan seleksi bahasa indonesia remove_inggris = [ w for w in filtered_words if not w in corpus_words ] #melakukan perulangan penghapusan text bahasa inggris gabung = \" \" . join ( remove_inggris ) #penggabungan kembali dari tokenizing 4. Stemming \u00b6 Tahap dimana menghilangkan infleksi atau kata imbuh ke bentuk kata dasar from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #----------- Step4 Stemming / menghilangkan infleksi atau kata imbuhan ke bentuk kata dasar ------------ factory = StemmerFactory () #stemming menggunakan library sastrawi (bahasa indonesia) stemmer = factory . create_stemmer () #memanggil fungsi untuk membuat factory hasil = stemmer . stem ( gabung ) #melakukan stemming dan menyimpan ke variabel Source \u00b6 Apabila ingin melihat seluruh file source silahkan klik di sini Referensi \u00b6 https://hendroprasetyo.com/apa-itu-preprocessing/#.YMIMcDoxXMU https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe","title":"II. Text Preprocessing"},{"location":"semester-8/webmining/mt2/#ringkasan-materi-ke-2","text":"Pada perkuliahan pertemuan keempat saya mendapatkan materi tentang Text Preprocessing . didalam perkuliahan ini saya diajarkan tentang bagaimana cara membersihkan sebuah data text agar menjadi data yang dapat diolah lebih lanjut.","title":"Ringkasan Materi ke-2"},{"location":"semester-8/webmining/mt2/#text-preprocessing","text":"Text preprocessing merupakan tahapan awal yang sangat penting sebelum melakukan proses ketahap lebih lanjut seperti sentimen analisis, topic modeling, dll. Data yang digunakan dalam proses mining tidak selamanya dalam kondisi yang ideal untuk diproses. Terkadang pada data tersebut terdapat berbagai permasalahan yang dapat menggangu hasil dari proses mining itu sendiri seperi diantaranya adalah missing value, data redundant, outliers, ataupun format data yang tidak sesuai dengan sistem. Oleh karenanya untuk mengatasi permasalahan tersebut dibutuhkan tahap Preprocessing. Preprocessing merupakan salah satu tahapan menghilangkan permasalahan-permasalahan yang dapat mengganggu hasil daripada proses data.","title":"Text Preprocessing"},{"location":"semester-8/webmining/mt2/#1-case-folding","text":"Case folding adalah salah satu bentuk teks preprocessing yang paling sederhana. Dimana didalam case folding bertujuan untuk merubah semua huruf dalam dokumen menjadi huruf kecil, serta dapat juga digunakan untuk menghapus tanda baca dan angka yang tidak dibutuhkan pada saat proses case folding.","title":"1. Case Folding"},{"location":"semester-8/webmining/mt2/#2-tokenizing","text":"Tokenizing adalah proses pemisahan teks kalimat menjadi potongan-potongan kata yang disebut sebagai token untuk kemudian dapat di proses lanjut dengan stopwords.","title":"2. Tokenizing"},{"location":"semester-8/webmining/mt2/#3-stopword","text":"Stopword adalah tahap mengambil kata-kata penting dari hasil token dengan menggunakan algoritma spotlist (membuang kata kurang penting). Stopword juga dapat menghapus kata umum yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna seperti (yang, dan, di, dari. dll).","title":"3. Stopword"},{"location":"semester-8/webmining/mt2/#4-stemming","text":"Stemming adalah proses menghilangkan infleksi kata menjadi kata dasarnya, misalnya banyak penggunaan penambahan kalimat. Misalnya kata \u201cmendengarkan\u201d, \u201cdengarkan\u201d, \u201cdidengarkan\u201d akan ditransformasi menjadi kata \u201cdengar\u201d.","title":"4. Stemming"},{"location":"semester-8/webmining/mt2/#tugas-pada-perkuliahan-saat-itu-saya-mendapatkan-tugas-sebagai-berikut","text":"Melakukan text preprocessing setelah melakukan tugas scraping website berita.","title":"Tugas pada perkuliahan saat itu saya mendapatkan tugas sebagai berikut:"},{"location":"semester-8/webmining/mt2/#alat-dan-bahan","text":"pada tugas ini saya menggunakan alamat website suarasurabaya untuk target mengambil data berita politik dan untuk mempermudah dalam pembuatan program python nya nanti perlu disiapkan library python yang dapat di download secara gratis. dalam tugas ini saya menggunakan library python sebagai berikut: re, library python yang berguna untuk penggunaan regular expression string, library python yang berguna untuk manipulasi tipe data string nltk.tokenize word_tokenize, library python yang berguna untuk melakukan tokenize setiap kata nltk.corpus stopwords, library python yang berguna untuk melakukan penghapusan kata yang tidak penting sesuai kata wordlist nltk.corpus words, library python berupa wordlist yang akan digunakan untuk stopwords sastrawi.stemmer, library python berupa wordlist bahasa indonesia yg digunakan pada saat proses stemm","title":"Alat dan Bahan"},{"location":"semester-8/webmining/mt2/#1-case-folding_1","text":"Disini saya akan menggunakan library re, dan string untuk proses manipulasi semua teks import re import string #----------- Step1 Case Folding merubah atau menghilangkan karakter yang tidak perlu ------------ text . lower () #merubah semua huruf ke huruf kecil text = re . sub ( r \"http\\S+|www\\S+|https\\S+\" , '' , text , flags = re . MULTILINE ) #menghapus urls text = re . sub ( r '\\d+' , '' , text ) #menghapus berdasarkan regular expression (angka 0 - 9) text = re . sub ( r '@[\\w_]+|#+[\\w_]+' , '' , text ) #menghapus user @ references and '#' from tag text = text . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) #menghapus tanda baca ,.dsb text = text . strip () #menghapus karakter kosong / double space","title":"1. Case Folding"},{"location":"semester-8/webmining/mt2/#2-tokenizing_1","text":"Disini saya akan menggunakan library nltk word_tokenize, yang berguna untuk melakukan tokenize menjadi tiap kata from nltk.tokenize import word_tokenize #----------- Step2 Tokenizing data / memisahkan setiap kata ------------ tweet_tokens = word_tokenize ( text ) #tokenizing kata dari data text dengan menggunakan nltk tokenizing","title":"2. Tokenizing"},{"location":"semester-8/webmining/mt2/#3-stopword_1","text":"Pada tahap ini akan dilakukan proses stopword / filtering kata yang tidak penting from nltk.corpus import stopwords from nltk.corpus import words stop_words = set ( stopwords . words ( 'indonesian' )) #mengatur seleksi text dalam bahasa indonesia menggunakan corpus corpus_words = set ( words . words ()) #mengatur seleksi text dalam bahasa inggris menggunakan corpus #----------- Step3 Filtering stopword / penghapusan kata yang tidak penting ------------ filtered_words = [ w for w in tweet_tokens if not w in stop_words ] #melakukan perulangan penghapusan text berdasarkan seleksi bahasa indonesia remove_inggris = [ w for w in filtered_words if not w in corpus_words ] #melakukan perulangan penghapusan text bahasa inggris gabung = \" \" . join ( remove_inggris ) #penggabungan kembali dari tokenizing","title":"3. Stopword"},{"location":"semester-8/webmining/mt2/#4-stemming_1","text":"Tahap dimana menghilangkan infleksi atau kata imbuh ke bentuk kata dasar from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #----------- Step4 Stemming / menghilangkan infleksi atau kata imbuhan ke bentuk kata dasar ------------ factory = StemmerFactory () #stemming menggunakan library sastrawi (bahasa indonesia) stemmer = factory . create_stemmer () #memanggil fungsi untuk membuat factory hasil = stemmer . stem ( gabung ) #melakukan stemming dan menyimpan ke variabel","title":"4. Stemming"},{"location":"semester-8/webmining/mt2/#source","text":"Apabila ingin melihat seluruh file source silahkan klik di sini","title":"Source"},{"location":"semester-8/webmining/mt2/#referensi","text":"https://hendroprasetyo.com/apa-itu-preprocessing/#.YMIMcDoxXMU https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe","title":"Referensi"},{"location":"semester-8/webmining/mt3/","text":"Ringkasan Materi ke-3 \u00b6 Pada perkuliahan pertemuan kelima dan keenam saya mendapatkan materi tentang Text Clustering dan Text Classification . didalam perkuliahan ini saya diajarkan tentang apa itu text clustering dan text classification Text Clustering \u00b6 Clustering dokumen adalah proses pengelompokan dokumen yang memiliki kesamaan topik. Tujuan dari proses clustering ini membagi dokumen berdasarkan kesamaan, sehingga memudahkan dalam proses pencarian. Clustering dokumen telah lama diterapkan untuk memudahkan pengguna dalam mencari dokumen. Penerapan clustering ini berstandar pada suatu hipotesis bahwa dokumen yang relevan akan cenderung berada pada cluster yang sama jika pada koleksi dokumen dilakukan clustering. Code Text Classification \u00b6 Klasifikasi teks adalah proses pemberian tag atau kategori ke teks menurut isinya. Klasifikasi teks dapat digunakan untuk mengatur, menyusun, dan mengkategorikan hampir semua hal. Misalnya, artikel baru dapat diatur berdasarkan topik, percakapan obrolan dapat diatur berdasarkan bahasa, penyebutan merek dapat diatur berdasarkan sentimen, dan sebagainya. Penelitian mengenai klasifikasi teks telah banyak dikembangkan dengan berbagai macam metode yang secara umum terbagi menjadi 3 kelompok yaitu: klasifikasi teks berbasis statistic: Naive Bayes,K-Nearest Neighbor, Category Center Vector, Support Vector Machine dan Maximum Entropy Model. klasifikasi teks berbasis koneksi: Artificial Neural Network. klasifikasi teks berbasis aturan (rule-based): Decision Tree. Berdasarkan penelitian oleh Yang Yiming dan Xin Liu pada tahun 1999. Metode klasifikasi berbasis statistik, terutama KNN dan SVM terbukti memiliki kinerja yang lebih baik dibandingkanlainnya. from sklearn.feature_selection import SelectKBest import pandas as pd from sklearn.feature_selection import mutual_info_classif selector = SelectKBest ( mutual_info_classif , k = 3 ) x_fitur_baru = selector . fit_transform ( x , y ) print ( 'Fitur yang dipilih' ) display ( x_fitur_baru ) x_fitur_baru . shape from sklearn.model_selection import train_test_split x_train , x_test , y_train , y_test = train_test_split ( x_fitur_baru , y , test_size = 0.2 , random_state = 0 ) from sklearn.neighbors import KNeighborsClassifier KNN = KNeighborsClassifier ( n_neighbors = 3 ) KNN . fit ( x_train , y_train ) y_pred = KNN . predict ( x_test ) from sklearn.metrics import classification_report , confusion_matrix , accuracy_score print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) print ( accuracy_score ( y_test , y_pred )) Source \u00b6 Apabila ingin melihat seluruh file source silahkan klik di sini Referensi \u00b6 http://ejournal.stikom-bali.ac.id/index.php/knsi/article/viewFile/445/98 https://mti.binus.ac.id/2020/09/03/klasifikasi-teks/","title":"III. Modelling"},{"location":"semester-8/webmining/mt3/#ringkasan-materi-ke-3","text":"Pada perkuliahan pertemuan kelima dan keenam saya mendapatkan materi tentang Text Clustering dan Text Classification . didalam perkuliahan ini saya diajarkan tentang apa itu text clustering dan text classification","title":"Ringkasan Materi ke-3"},{"location":"semester-8/webmining/mt3/#text-clustering","text":"Clustering dokumen adalah proses pengelompokan dokumen yang memiliki kesamaan topik. Tujuan dari proses clustering ini membagi dokumen berdasarkan kesamaan, sehingga memudahkan dalam proses pencarian. Clustering dokumen telah lama diterapkan untuk memudahkan pengguna dalam mencari dokumen. Penerapan clustering ini berstandar pada suatu hipotesis bahwa dokumen yang relevan akan cenderung berada pada cluster yang sama jika pada koleksi dokumen dilakukan clustering. Code","title":"Text Clustering"},{"location":"semester-8/webmining/mt3/#text-classification","text":"Klasifikasi teks adalah proses pemberian tag atau kategori ke teks menurut isinya. Klasifikasi teks dapat digunakan untuk mengatur, menyusun, dan mengkategorikan hampir semua hal. Misalnya, artikel baru dapat diatur berdasarkan topik, percakapan obrolan dapat diatur berdasarkan bahasa, penyebutan merek dapat diatur berdasarkan sentimen, dan sebagainya. Penelitian mengenai klasifikasi teks telah banyak dikembangkan dengan berbagai macam metode yang secara umum terbagi menjadi 3 kelompok yaitu: klasifikasi teks berbasis statistic: Naive Bayes,K-Nearest Neighbor, Category Center Vector, Support Vector Machine dan Maximum Entropy Model. klasifikasi teks berbasis koneksi: Artificial Neural Network. klasifikasi teks berbasis aturan (rule-based): Decision Tree. Berdasarkan penelitian oleh Yang Yiming dan Xin Liu pada tahun 1999. Metode klasifikasi berbasis statistik, terutama KNN dan SVM terbukti memiliki kinerja yang lebih baik dibandingkanlainnya. from sklearn.feature_selection import SelectKBest import pandas as pd from sklearn.feature_selection import mutual_info_classif selector = SelectKBest ( mutual_info_classif , k = 3 ) x_fitur_baru = selector . fit_transform ( x , y ) print ( 'Fitur yang dipilih' ) display ( x_fitur_baru ) x_fitur_baru . shape from sklearn.model_selection import train_test_split x_train , x_test , y_train , y_test = train_test_split ( x_fitur_baru , y , test_size = 0.2 , random_state = 0 ) from sklearn.neighbors import KNeighborsClassifier KNN = KNeighborsClassifier ( n_neighbors = 3 ) KNN . fit ( x_train , y_train ) y_pred = KNN . predict ( x_test ) from sklearn.metrics import classification_report , confusion_matrix , accuracy_score print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) print ( accuracy_score ( y_test , y_pred ))","title":"Text Classification"},{"location":"semester-8/webmining/mt3/#source","text":"Apabila ingin melihat seluruh file source silahkan klik di sini","title":"Source"},{"location":"semester-8/webmining/mt3/#referensi","text":"http://ejournal.stikom-bali.ac.id/index.php/knsi/article/viewFile/445/98 https://mti.binus.ac.id/2020/09/03/klasifikasi-teks/","title":"Referensi"}]}